{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c408ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "from models.trajfm import TrajFM\n",
    "from data import TrajFMDataset, fetch_task_padder, X_COL, Y_COL, coord_transform_GPS_UTM\n",
    "from torch.utils.data import random_split\n",
    "import warnings\n",
    "from pipeline import train_user_model, test_user_model\n",
    "import torch.multiprocessing as mp\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84217ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS_CACHE_DIR = os.environ.get('SETTINGS_CACHE_DIR', os.path.join('settings', 'cache'))\n",
    "MODEL_CACHE_DIR = os.environ.get('MODEL_CACHE_DIR', 'saved_model')\n",
    "LOG_SAVE_DIR = os.environ.get('LOG_SAVE_DIR', 'logs')\n",
    "PRED_SAVE_DIR = os.environ.get('PRED_SAVE_DIR', 'predictions')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "\n",
    "if mp.get_start_method(allow_none=True) is None:\n",
    "    mp.set_start_method('spawn')\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "# This key is an indicator of multiple things.\n",
    "datetime_key = utils.get_datetime_key()\n",
    "\n",
    "\n",
    "with open(os.path.join('settings', f'local_test.json'), 'r') as fp:\n",
    "    setting = json.load(fp)\n",
    "    setting = setting[0]\n",
    "utils.create_if_noexists(SETTINGS_CACHE_DIR)\n",
    "with open(os.path.join(SETTINGS_CACHE_DIR, f'{datetime_key}.json'), 'w') as fp:\n",
    "    json.dump(setting, fp)\n",
    "    \n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71773187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: ignore[reportIndexIssue]\n",
    "SAVE_NAME = setting[\"save_name\"]\n",
    "\n",
    "train_traj_df = pd.read_hdf(setting['dataset']['train_traj_df'], key='trips')\n",
    "print(\"dataset:\", setting['dataset']['train_traj_df'])\n",
    "user_count = len(train_traj_df['user_id'].unique())\n",
    "traj_count = len(train_traj_df['traj_id'].unique())\n",
    "traj_len = len(train_traj_df['seq_i'])\n",
    "tao = train_traj_df['delta_t'].mean()\n",
    "setting[\"finetune\"][\"padder\"][\"params\"][\"num_users\"] = user_count\n",
    "\n",
    "scale = 4000\n",
    "# if \"chengdu\" in setting['dataset']['train_traj_df']:\n",
    "#     UTM_region = 48\n",
    "# if \"xian\" in setting['dataset']['train_traj_df']:\n",
    "#     UTM_region = 49\n",
    "if \"geolife\" in setting['dataset']['train_traj_df']:\n",
    "    UTM_region = 50  \n",
    "train_dataset = TrajFMDataset(traj_df=train_traj_df, UTM_region=UTM_region, scale = scale)\n",
    "\n",
    "poi_df = pd.read_hdf(setting['dataset']['poi_df'], key='pois')\n",
    "poi_embed = torch.from_numpy(np.load(setting['dataset']['poi_embed'])).float().to(device)\n",
    "\n",
    "poi_coors = poi_df[[X_COL, Y_COL]].to_numpy()\n",
    "poi_coors = (coord_transform_GPS_UTM(poi_coors, UTM_region) - train_dataset.spatial_middle_coord) / scale\n",
    "poi_coors = torch.tensor(poi_coors).float().to(device)\n",
    "\n",
    "# Build the learnable model.\n",
    "trajfm = TrajFM(poi_embed=poi_embed, \n",
    "                poi_coors=poi_coors, \n",
    "                UTM_region=UTM_region,\n",
    "                spatial_middle_coord = train_dataset.spatial_middle_coord, \n",
    "                scale = scale, \n",
    "                **setting['trajfm'],\n",
    "                user = user_count).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62886f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = {\n",
    "    \"users\": user_count,\n",
    "    \"total_traj\": traj_count,           \n",
    "    \"total_points\": traj_len,\n",
    "    \"avg_traj_len\": f\"{round(tao/3600, 2)} hours\",\n",
    "    \"Data Filtering\": [ \n",
    "        \"25th to 75th quartile based on traj_len\", \n",
    "        \"traj_len > 30 points\", \n",
    "        \"delta_t > 1800s\",\n",
    "        \"traj/user > 35 traj\",\n",
    "        \"resampled traj to 1000 points max\",\n",
    "        \"user_number and seq_i recalculated\",]\n",
    "}\n",
    "\n",
    "for key, value in data_summary.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20c8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train_dataset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_test_size = total_size - train_size\n",
    "val_size = int(0.5 * val_test_size)\n",
    "test_size = val_test_size - val_size\n",
    "\n",
    "train_dataset, val_test_dataset = random_split(train_dataset, [train_size, val_test_size])\n",
    "val_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56d6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "downstreamtask = setting['finetune']['padder']['name']    \n",
    "padder = fetch_task_padder(padder_name=setting['finetune']['padder']['name'], padder_params=setting['finetune']['padder']['params'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=padder, **setting['finetune']['dataloader'])\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=padder, **setting['finetune']['dataloader'])\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=padder, **setting['finetune']['dataloader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca23fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"saved_model/{SAVE_NAME}.{downstreamtask}\"\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"Loading model {file_path}\")\n",
    "    trajfm.load_state_dict(torch.load(os.path.join(MODEL_CACHE_DIR, f'{SAVE_NAME}.{downstreamtask}'), map_location=device))\n",
    "else:\n",
    "    print(\"Model not found, starting new\")\n",
    "\n",
    "train_log, saved_model_state_dict = train_user_model(model=trajfm, \n",
    "                                                    train_dataloader=train_dataloader, \n",
    "                                                    val_dataloader=val_dataloader,\n",
    "                                                    device = device, \n",
    "                                                    **setting['finetune']['config'],\n",
    "                                                    data_summary = data_summary)\n",
    "\n",
    "if setting['finetune'].get('save', False):\n",
    "    # save model\n",
    "    utils.create_if_noexists(MODEL_CACHE_DIR)\n",
    "    torch.save(saved_model_state_dict, os.path.join(MODEL_CACHE_DIR, f'{SAVE_NAME}.{downstreamtask}'))\n",
    "    \n",
    "    # save log\n",
    "    log_dir = os.path.join(LOG_SAVE_DIR, SAVE_NAME)\n",
    "    utils.create_if_noexists(log_dir)\n",
    "    log_path = os.path.join(log_dir, f'{SAVE_NAME}_{downstreamtask}.csv')\n",
    "    file_exists = os.path.exists(log_path)\n",
    "    train_log.to_csv(log_path, mode='a', header=not file_exists, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdedca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, _ = test_user_model(model=trajfm, dataloader=test_dataloader, device = device)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {round(value * 100, 2)}%,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f798cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{\n",
    "    \"Model\": f\"{SAVE_NAME}\",\n",
    "    **{key: round(value * 100, 2) for key, value in metrics.items()}\n",
    "}])\n",
    "\n",
    "csv_path = \"logs/test.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    df.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
